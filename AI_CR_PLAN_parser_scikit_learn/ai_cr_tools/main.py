#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIä»£ç å¼‚å‘³æ£€æµ‹ä¸»ç¨‹åº
åŸºäºphp-parser + scikit-learnçš„ä»£ç å¼‚å‘³æ£€æµ‹ç³»ç»Ÿ
"""

import os
import sys
import argparse
import glob
import json
from typing import List
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from code_smell_detector import CodeSmellDetector
from model_trainer import ModelTrainer
from feature_extractor import FeatureExtractor
from php_parser import PHPParser

def find_php_files(directory: str, recursive: bool = True) -> List[str]:
    """
    æŸ¥æ‰¾ç›®å½•ä¸­çš„PHPæ–‡ä»¶
    
    Args:
        directory: æœç´¢ç›®å½•
        recursive: æ˜¯å¦é€’å½’æœç´¢
        
    Returns:
        PHPæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    php_files = []
    
    if recursive:
        pattern = os.path.join(directory, "**", "*.php")
        php_files = glob.glob(pattern, recursive=True)
    else:
        pattern = os.path.join(directory, "*.php")
        php_files = glob.glob(pattern)
    
    return php_files

def detect_command(args):
    """æ£€æµ‹å‘½ä»¤å¤„ç†"""
    print("ğŸ” å¼€å§‹ä»£ç å¼‚å‘³æ£€æµ‹...")
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = CodeSmellDetector(models_dir=args.models_dir)
    
    # ç¡®å®šè¦æ£€æµ‹çš„æ–‡ä»¶
    php_files = []
    
    if args.file:
        # æ£€æµ‹å•ä¸ªæ–‡ä»¶
        if not os.path.exists(args.file):
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {args.file}")
            return
        php_files = [args.file]
    elif args.directory:
        # æ£€æµ‹ç›®å½•ä¸­çš„æ–‡ä»¶
        if not os.path.exists(args.directory):
            print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ {args.directory}")
            return
        php_files = find_php_files(args.directory, args.recursive)
    else:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šè¦æ£€æµ‹çš„æ–‡ä»¶æˆ–ç›®å½•")
        return
    
    if not php_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°PHPæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(php_files)} ä¸ªPHPæ–‡ä»¶")
    
    # æ‰¹é‡æ£€æµ‹
    results = detector.detect_batch(php_files)
    
    if not results:
        print("âŒ æ£€æµ‹å¤±è´¥")
        return
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print(f"\nğŸ“Š æ£€æµ‹å®Œæˆï¼Œå…±æ£€æµ‹ {len(results)} ä¸ªæ–‡ä»¶")
    
    # ç»Ÿè®¡å¼‚å‘³ç±»å‹
    smell_counts = {}
    issue_counts = {'error': 0, 'warning': 0, 'info': 0}
    
    for result in results:
        smell_type = result.smell_type
        smell_counts[smell_type] = smell_counts.get(smell_type, 0) + 1
        
        for issue in result.issues:
            severity = issue.get('severity', 'info')
            issue_counts[severity] += 1
    
    print("\nğŸ·ï¸  å¼‚å‘³ç±»å‹åˆ†å¸ƒ:")
    for smell_type, count in smell_counts.items():
        print(f"   {detector.get_smell_description(smell_type)} ({smell_type}): {count} ä¸ªæ–‡ä»¶")
    
    print(f"\nâš ï¸  é—®é¢˜ç»Ÿè®¡:")
    print(f"   é”™è¯¯: {issue_counts['error']} ä¸ª")
    print(f"   è­¦å‘Š: {issue_counts['warning']} ä¸ª")  
    print(f"   æç¤º: {issue_counts['info']} ä¸ª")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœï¼ˆå¦‚æœå¯ç”¨è¯¦ç»†æ¨¡å¼ï¼‰
    if args.verbose:
        print("\nğŸ“„ è¯¦ç»†ç»“æœ:")
        for result in results:
            print(f"\nğŸ“ {os.path.basename(result.file_path)}")
            print(f"   å¼‚å‘³ç±»å‹: {detector.get_smell_description(result.smell_type)} ({result.smell_type}) (ç½®ä¿¡åº¦: {result.confidence:.2%})")
            
            if result.issues:
                print("   é—®é¢˜:")
                for issue in result.issues:
                    severity_icon = {'error': 'ğŸš¨', 'warning': 'âš ï¸', 'info': 'â„¹ï¸'}.get(issue['severity'], 'â„¹ï¸')
                    print(f"   {severity_icon} {issue['message']}: {issue['details']}")
            
            if result.suggestions and args.suggestions:
                print("   å»ºè®®:")
                for suggestion in result.suggestions:
                    print(f"   ğŸ’¡ {suggestion}")
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.output:
        detector.generate_report(results, args.output)
        print(f"\nğŸ“‹ æ£€æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")

def train_command(args):
    """è®­ç»ƒå‘½ä»¤å¤„ç†"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = ModelTrainer(models_dir=args.models_dir)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    if args.synthetic:
        print("ğŸ² ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®...")
        files, labels = trainer.generate_synthetic_data(args.samples)
    else:
        # ä»æŒ‡å®šç›®å½•æ”¶é›†çœŸå®æ•°æ®ï¼ˆéœ€è¦æ ‡æ³¨ï¼‰
        if not args.data_dir:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šè®­ç»ƒæ•°æ®ç›®å½•æˆ–ä½¿ç”¨ --synthetic ç”Ÿæˆåˆæˆæ•°æ®")
            return
        
        # è¿™é‡Œåº”è¯¥å®ç°ä»æ ‡æ³¨æ•°æ®åŠ è½½çš„é€»è¾‘
        print(f"ğŸ“‚ ä» {args.data_dir} åŠ è½½è®­ç»ƒæ•°æ®...")
        # TODO: å®ç°çœŸå®æ•°æ®åŠ è½½é€»è¾‘
        files, labels = trainer.generate_synthetic_data(args.samples)
        print("âš ï¸  å½“å‰ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º")
    
    try:
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X, y = trainer.prepare_training_data(files, labels)
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ”§ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        results = trainer.train_models(X, y, use_grid_search=args.grid_search)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        best_model = trainer.save_best_model(results)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹: {best_model}")
        print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½:")
        for model_name, result in results.items():
            print(f"   {model_name}:")
            print(f"     å‡†ç¡®ç‡: {result['accuracy']:.4f}")
            print(f"     äº¤å‰éªŒè¯: {result['cv_mean']:.4f} (Â±{result['cv_std']*2:.4f})")
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        if args.plot:
            plot_path = os.path.join(args.models_dir, 'training_results.png')
            trainer.plot_results(results, plot_path)
            print(f"ğŸ“Š è®­ç»ƒç»“æœå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if args.synthetic:
            import shutil
            temp_dir = os.path.dirname(files[0]) if files else None
            if temp_dir and os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                print(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")

def analyze_command(args):
    """åˆ†æå‘½ä»¤å¤„ç†"""
    print("ğŸ“Š å¼€å§‹ä»£ç åˆ†æ...")
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = FeatureExtractor()
    
    # ç¡®å®šè¦åˆ†æçš„æ–‡ä»¶
    if args.file:
        if not os.path.exists(args.file):
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {args.file}")
            return
        php_files = [args.file]
    elif args.directory:
        if not os.path.exists(args.directory):
            print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ {args.directory}")
            return
        php_files = find_php_files(args.directory, args.recursive)
    else:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šè¦åˆ†æçš„æ–‡ä»¶æˆ–ç›®å½•")
        return
    
    if not php_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°PHPæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(php_files)} ä¸ªPHPæ–‡ä»¶")
    
    # åˆ†ææ–‡ä»¶
    all_features = []
    for i, file_path in enumerate(php_files):
        try:
            features = extractor.extract_features(file_path)
            all_features.append((file_path, features))
            
            if args.verbose:
                print(f"\nğŸ“„ {os.path.basename(file_path)}:")
                print(f"   ä»£ç è¡Œæ•°: {features.lines_of_code}")
                print(f"   åœˆå¤æ‚åº¦: {features.cyclomatic_complexity}")
                print(f"   ç±»æ•°é‡: {features.number_of_classes}")
                print(f"   æ–¹æ³•æ•°é‡: {features.number_of_methods}")
                print(f"   å¹³å‡æ–¹æ³•å¤æ‚åº¦: {features.avg_method_complexity:.2f}")
                print(f"   å‘½åçº¦å®šè¿è§„: {features.naming_convention_violations}")
                print(f"   æ³¨é‡Šå¯†åº¦: {features.comment_ratio:.2%}")
            
            if (i + 1) % 10 == 0:
                print(f"å·²åˆ†æ {i + 1}/{len(php_files)} ä¸ªæ–‡ä»¶")
                
        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    if all_features:
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦ (åŸºäº {len(all_features)} ä¸ªæ–‡ä»¶):")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        lines = [f.lines_of_code for _, f in all_features]
        complexities = [f.cyclomatic_complexity for _, f in all_features]
        classes = [f.number_of_classes for _, f in all_features]
        methods = [f.number_of_methods for _, f in all_features]
        
        print(f"   å¹³å‡ä»£ç è¡Œæ•°: {sum(lines)/len(lines):.1f}")
        print(f"   å¹³å‡åœˆå¤æ‚åº¦: {sum(complexities)/len(complexities):.1f}")
        print(f"   å¹³å‡ç±»æ•°é‡: {sum(classes)/len(classes):.1f}")
        print(f"   å¹³å‡æ–¹æ³•æ•°é‡: {sum(methods)/len(methods):.1f}")
        
        # æ‰¾å‡ºé—®é¢˜æ–‡ä»¶
        problematic_files = []
        for file_path, features in all_features:
            issues = []
            if features.lines_of_code > 1000:
                issues.append("æ–‡ä»¶è¿‡å¤§")
            if features.cyclomatic_complexity > 20:
                issues.append("å¤æ‚åº¦è¿‡é«˜")
            if features.long_method_count > 0:
                issues.append("å­˜åœ¨é•¿æ–¹æ³•")
            if features.naming_convention_violations > 5:
                issues.append("å‘½åçº¦å®šè¿è§„")
            
            if issues:
                problematic_files.append((file_path, issues))
        
        if problematic_files:
            print(f"\nâš ï¸  å‘ç° {len(problematic_files)} ä¸ªé—®é¢˜æ–‡ä»¶:")
            for file_path, issues in problematic_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   ğŸ“ {os.path.basename(file_path)}: {', '.join(issues)}")
            
            if len(problematic_files) > 5:
                print(f"   ... è¿˜æœ‰ {len(problematic_files) - 5} ä¸ªæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="AIä»£ç å¼‚å‘³æ£€æµ‹å·¥å…· - åŸºäºphp-parser + scikit-learn",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ£€æµ‹å•ä¸ªæ–‡ä»¶
  python main.py detect -f path/to/file.php
  
  # æ£€æµ‹æ•´ä¸ªç›®å½•
  python main.py detect -d /path/to/project --recursive
  
  # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
  python main.py detect -d /path/to/project -o report.html --verbose
  
  # è®­ç»ƒæ¨¡å‹
  python main.py train --synthetic --samples 500 --grid-search
  
  # åˆ†æä»£ç ç»Ÿè®¡ä¿¡æ¯
  python main.py analyze -d /path/to/project --verbose
        """
    )
    
    # å…¨å±€å‚æ•°
    parser.add_argument('--models-dir', default='models', 
                       help='æ¨¡å‹æ–‡ä»¶ç›®å½• (é»˜è®¤: models)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ£€æµ‹å‘½ä»¤
    detect_parser = subparsers.add_parser('detect', help='æ£€æµ‹ä»£ç å¼‚å‘³')
    detect_group = detect_parser.add_mutually_exclusive_group(required=True)
    detect_group.add_argument('-f', '--file', help='è¦æ£€æµ‹çš„PHPæ–‡ä»¶')
    detect_group.add_argument('-d', '--directory', help='è¦æ£€æµ‹çš„ç›®å½•')
    detect_parser.add_argument('-r', '--recursive', action='store_true',
                              help='é€’å½’æœç´¢ç›®å½•')
    detect_parser.add_argument('-o', '--output', default='code_smell_report.html',
                              help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶ (é»˜è®¤: code_smell_report.html)')
    detect_parser.add_argument('--suggestions', action='store_true',
                              help='æ˜¾ç¤ºæ”¹è¿›å»ºè®®')
    
    # è®­ç»ƒå‘½ä»¤
    train_parser = subparsers.add_parser('train', help='è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹')
    train_parser.add_argument('--synthetic', action='store_true',
                             help='ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒ')
    train_parser.add_argument('--data-dir', help='è®­ç»ƒæ•°æ®ç›®å½•')
    train_parser.add_argument('--samples', type=int, default=500,
                             help='åˆæˆæ•°æ®æ ·æœ¬æ•°é‡ (é»˜è®¤: 500)')
    train_parser.add_argument('--grid-search', action='store_true',
                             help='ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–å‚æ•°')
    train_parser.add_argument('--plot', action='store_true',
                             help='ç”Ÿæˆè®­ç»ƒç»“æœå›¾è¡¨')
    
    # åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æä»£ç ç»Ÿè®¡ä¿¡æ¯')
    analyze_group = analyze_parser.add_mutually_exclusive_group(required=True)
    analyze_group.add_argument('-f', '--file', help='è¦åˆ†æçš„PHPæ–‡ä»¶')
    analyze_group.add_argument('-d', '--directory', help='è¦åˆ†æçš„ç›®å½•')
    analyze_parser.add_argument('-r', '--recursive', action='store_true',
                               help='é€’å½’æœç´¢ç›®å½•')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    os.makedirs(args.models_dir, exist_ok=True)
    
    # æ‰§è¡Œå¯¹åº”å‘½ä»¤
    try:
        if args.command == 'detect':
            detect_command(args)
        elif args.command == 'train':
            train_command(args)
        elif args.command == 'analyze':
            analyze_command(args)
    except KeyboardInterrupt:
        print("\nâŒ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()

if __name__ == '__main__':
    main() 